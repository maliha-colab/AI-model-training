# -*- coding: utf-8 -*-
"""model training .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xcsQGR6JG0OW8z73NZ-QXkkGOSAPrJ_V
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from datasets import Dataset
import torch
import numpy as np

df = pd.read_csv('/content/train_v1_updated.csv')

sentiment_labels = {
    'mycket negativ': 0,
    'lite negativ': 1,
    'neutral': 2,
    'lite positiv': 3,
    'mycket positiv': 4
}

df['label'] = df['label'].map(sentiment_labels)

df.dropna(subset=['label'], inplace=True)
df['label'] = df['label'].astype(int)

train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])

train_dataset = Dataset.from_pandas(train_df)
val_dataset = Dataset.from_pandas(val_df)

token = "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"

import os
os.environ["HF_TOKEN"] = "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"

from transformers import AutoTokenizer, AutoModel

model_name = "HI-84/HI_model"
token = "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"

tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)
model = AutoModel.from_pretrained(model_name, token=token)

print("Model loaded successfully!")

AutoTokenizer.from_pretrained(model_name, token=os.environ["HF_TOKEN"])

model_name = "HI-84/HI_model"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(sentiment_labels))

def tokenize_function(examples):
    return tokenizer(examples['text'], padding="max_length", truncation=True)

tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)
tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)

def tokenize(batch):
    return tokenizer(
        batch["text"],
        padding="max_length",
        truncation=True,
        max_length=128
    )

tokenized_train_dataset.set_format("torch")
tokenized_val_dataset.set_format("torch")

from torch.utils.data import DataLoader, RandomSampler, WeightedRandomSampler
from torch.nn import CrossEntropyLoss

class CustomTrainer(Trainer):
    def _init_(self, *args, class_weights=None, oversample_classes=None, **kwargs):
        super()._init_(*args, **kwargs)
        self.class_weights = class_weights
        self.oversample_classes = oversample_classes

    def get_train_dataloader(self):
        if self.oversample_classes:
            labels = np.array(self.train_dataset['label'])
            class_counts = np.bincount(labels)
            weights = 1. / class_counts

            if self.class_weights is not None:
                for i, weight_val in enumerate(self.class_weights):
                    weights[i] *= weight_val

            sample_weights = weights[labels]
            sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)
            return DataLoader(self.train_dataset, batch_size=self.args.per_device_train_batch_size, sampler=sampler)
        else:
            return super().get_train_dataloader()

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=100,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10
)

from datasets import Dataset
hf_dataset = Dataset.from_pandas(df)

def tokenize(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True)
tokenized_df = hf_dataset.map(tokenize, batched=True)

from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

tokenized_train = train_dataset.map(tokenize, batched=True)
tokenized_test = test_dataset.map(tokenize, batched=True)

!pip install -q transformers datasets

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import numpy as np
from sklearn.metrics import accuracy_score

tokenizer = AutoTokenizer.from_pretrained("HI-84/HI_model")

def tokenize(batch):
   return tokenizer(batch["text"], padding="max_length", truncation=True)

tokenized_df = tokenized_df.map(tokenize, batched=True)
tokenized_df = tokenized_df.remove_columns(["text"])

model = AutoModelForSequenceClassification.from_pretrained("HI-84/HI_model", num_labels=2)

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=100,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10
)

def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc}

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train.shuffle(seed=42).select(range(2000)),
    eval_dataset=tokenized_test.select(range(500)),
    compute_metrics=compute_metrics
)

model = AutoModelForSequenceClassification.from_pretrained("HI-84/HI_model", num_labels=5)

df['label'].value_counts()
print("Unique labels:", df['label'].nunique())

def tokenize_function(example):
    return tokenizer(
        example["text"],
        padding="max_length",
        truncation=True,
        max_length=128
    )

df.columns

train_df = train_df.rename(columns={"sentence": "text"})
val_df = val_df.rename(columns={"sentence": "text"})

from datasets import Dataset

train_dataset = Dataset.from_pandas(train_df)
val_dataset = Dataset.from_pandas(val_df)

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)
tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)

tokenized_train_dataset = tokenized_train_dataset.remove_columns(["text", "__index_level_0__"])
tokenized_val_dataset = tokenized_val_dataset.remove_columns(["text", "__index_level_0__"])

tokenized_train_dataset.set_format("torch")
tokenized_val_dataset.set_format("torch")

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    logging_dir="./logs",
    logging_steps=10
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_val_dataset,
    tokenizer=tokenizer,
)

train_dataset.column_names

trainer.train()

output_model_dir = "./swedbert_sentiment_model"
model.save_pretrained(output_model_dir)
tokenizer.save_pretrained(output_model_dir)

print(f"Model and tokenizer saved to {output_model_dir}")

# Basic ML backend placeholder (swedbert_backend.py)
# This would typically be a Flask/FastAPI app. For now, just a conceptual outline.
backend_code = """
from flask import Flask, request, jsonify
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

app = Flask(__name__)

# Load the fine-tuned model and tokenizer
model_path = "./swedbert_sentiment_model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)
model.eval() # Set model to evaluation mode

sentiment_map = {
    0: 'mycket negativ',
    1: 'lite negativ',
    2: 'neutral',
    3: 'lite positiv',
    4: 'mycket positiv'
}
# ✅ Add this route BEFORE app.run()
@app.route("/")
def index():
    return "✅ Flask ML backend is running!"

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    text = data.get('text', '')

    if not text:
        return jsonify({'error': 'No text provided'}), 400

    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probabilities = torch.softmax(logits, dim=1)
        predicted_class_id = torch.argmax(probabilities, dim=1).item()
        score = probabilities[0][predicted_class_id].item()

    predicted_label = sentiment_map[predicted_class_id]

    # Apply the post-processing logic for filtering
    # non-neutral rows with score >= 0.8
    # neutral rows only if score >= 0.9

    # Convert predicted_class_id back to score -2 to +2
    # 0: -2, 1: -1, 2: 0, 3: 1, 4: 2
    sentiment_score = predicted_class_id - 2

    # This filtering logic is for *after* the model delivers, during auto-labeling
    # For the prediction endpoint, we should return the raw prediction and probability
    # The filtering will be done by the client consuming this API

    return jsonify({
        'text': text,
        'predicted_label': predicted_label,
        'probability': score,
        'sentiment_score': sentiment_score
    })

if __name__ == '__main__':
    # This will run on port 9090 as specified in the agreement
    app.run(host='0.0.0.0', port=9090)
"""

with open("swedbert_backend.py", "w") as f:
    f.write(backend_code)

print("swedbert_backend.py created.")

# Instructions for Google Colab
colab_instructions = """
# Google Colab Setup and Execution Instructions

This script `train_swedbert.py` will fine-tune the SwedBERT model for sentiment analysis.

**1. Upload Files:**
   - Upload `train_v1_updated.csv` and `train_swedbert.py` to your Google Colab environment.

**2. Install Dependencies:**
   Run the following in a Colab cell to install necessary libraries:
   ```bash
   !pip install transformers datasets scikit-learn pandas torch flask
   ```

**3. Run the Training Script:**
   Run the training script from a Colab cell:
   ```bash
   !python train_swedbert.py
   ```

   This will:
   - Load and preprocess your data.
   - Fine-tune the SwedBERT model.
   - Save the trained model and tokenizer to `./swedbert_sentiment_model/`.
   - Create a placeholder Flask backend script `swedbert_backend.py`.

**4. Deploying the ML Backend (Conceptual):**
   The `swedbert_backend.py` is a basic Flask application. To deploy it for Label Studio or a public API, you would typically use a service like Ngrok (for temporary local exposure) or a cloud platform (e.g., Google Cloud Run, AWS EC2, Azure App Service) for production deployment.

   **To run the backend locally in Colab (for testing purposes, requires port forwarding/tunneling like ngrok):**
   ```bash
   !python swedbert_backend.py
   ```
   *Note: Directly exposing ports from Colab requires additional tools like `ngrok` or `localtunnel` for external access, which is beyond the scope of this script but essential for Label Studio to connect.*

**5. Retraining:**
   To retrain with new data, simply replace `train_v1_updated.csv` with your new `gold-label CSV` and re-run `!python train_swedbert.py`.

**Important Considerations:**
- **GPU Usage:** Ensure you are running your Colab notebook with a GPU runtime (`Runtime -> Change runtime type -> GPU`).
- **Data Volume:** For very large datasets, consider using `Trainer`'s `fp16=True` argument in `TrainingArguments` for mixed-precision training to save memory.
- **Hyperparameters:** The `num_train_epochs`, `per_device_train_batch_size`, etc., in `TrainingArguments` can be tuned for better performance.
- **Oversampling/Class Weights:** The `CustomTrainer` implements the specified logic. Monitor training logs to ensure it's having the desired effect on class balance.
"""

with open("colab_instructions.md", "w") as f:
    f.write(colab_instructions)

print("colab_instructions.md created.")

import pandas as pd
import requests
import json
from datetime import datetime
SOCIAL_MEDIA_API_ENDPOINT = "https://api.example.com/social_media_feed"
API_KEY = "YOUR_SOCIAL_MEDIA_API_KEY"
def fetch_social_media_data(query, num_posts=100):
    """
    Fetches social media posts based on a query.
    In a real application, this would interact with a social media API.
    """
    print(f"Fetching {num_posts} posts for query: ", query)
    mock_data = []
    for i in range(num_posts):
        mock_data.append({
            "id": f"post_{datetime.now().strftime('%Y%m%d%H%M%S%f')}_{i}",
            "text": f"This is a sample post about {query}. Sentiment is neutral for now. Post number {i}.",
            "timestamp": datetime.now().isoformat()
        })
    return mock_data

def preprocess_post(post_text):
    processed_text = post_text.replace("http://", "").replace("https://", "")
    processed_text = ' '.join(word for word in processed_text.split() if not word.startswith(('@', '#')))
    return processed_text.strip()
if __name__ == "__main__":
    stocks_to_monitor = ["Microsoft", "Ericsson", "Volvo"]
    all_processed_data = []

    for stock in stocks_to_monitor:
        raw_posts = fetch_social_media_data(stock, num_posts=50)
        for post in raw_posts:
            processed_text = preprocess_post(post["text"])
            all_processed_data.append({
                "stock": stock,
                "original_text": post["text"],
                "processed_text": processed_text,
                "timestamp": post["timestamp"]
            })

    df_collected = pd.DataFrame(all_processed_data)
    output_filename = "collected_social_media_data.csv"
    df_collected.to_csv(output_filename, index=False)
    print(f"Collected and preprocessed data saved to {output_filename}")

    print("\n--- Demonstrating sending data to ML Backend (conceptual) ---")
    if not df_collected.empty:
        sample_post = df_collected.iloc[0]
        ml_backend_url = "http://localhost:9090/predict"

        try:
            print(f"Sending post for prediction: {sample_post['processed_text']}")
            response = requests.post(ml_backend_url, json={'text': sample_post['processed_text']})
            response.raise_for_status()
            prediction_result = response.json()
            print("Prediction Result:", prediction_result)
        except requests.exceptions.ConnectionError:
            print(f"Error: Could not connect to ML backend at {ml_backend_url}. Is it running?")
        except requests.exceptions.HTTPError as e:
            print(f"HTTP Error: {e.response.status_code} - {e.response.text}")
        except Exception as e:
            print(f"An unexpected error occurred: {e}")
    else:
        print("No data collected to send for prediction.")

!ngrok config add-authtoken 30wuuaw5Z7ScSCRcqOAZTy4gNpf_2dnTwo9q34XTpMjTKs6fx

!pip install flask-ngrok pyngrok transformers torch --quiet

from flask import Flask, request, jsonify
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from pyngrok import ngrok
import threading
app = Flask(__name__)

model_path = "./swedbert_sentiment_model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)
model.eval()

sentiment_map = {
    0: 'mycket negativ',
    1: 'lite negativ',
    2: 'neutral',
    3: 'lite positiv',
    4: 'mycket positiv'
}
@app.route("/")
def index():
    return " Flask ML backend is running!"
@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    text = data.get('text', '')

    if not text:
        return jsonify({'error': 'No text provided'}), 400

    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probabilities = torch.softmax(logits, dim=1)
        predicted_class_id = torch.argmax(probabilities, dim=1).item()
        score = probabilities[0][predicted_class_id].item()

    predicted_label = sentiment_map[predicted_class_id]
    sentiment_score = predicted_class_id - 2

    return jsonify({
        'text': text,
        'predicted_label': predicted_label,
        'probability': score,
        'sentiment_score': sentiment_score
    })

def run_flask():
    app.run(port=9090)

thread = threading.Thread(target=run_flask)
thread.start()


public_url = ngrok.connect(9090)
print(f" Public URL for prediction: {public_url}/predict")

model.save_pretrained("swedbert_sentiment_model")
tokenizer.save_pretrained("swedbert_sentiment_model")

from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("swedbert_sentiment_model")
tokenizer = AutoTokenizer.from_pretrained("swedbert_sentiment_model")
inputs = tokenizer("Jag tycker det är väldigt bra.", return_tensors="pt")
outputs = model(**inputs)
print(outputs.logits.argmax().item())

!pip install pyngrok

public_url = ngrok.connect(9090)
print("Public URL:", public_url)

!python swedbert_backend.py

!python train_swedbert.py

!pip install numpy==1.26.4

model_path = "SwedBERT-sentiment-v1"
trainer.save_model(model_path)
tokenizer.save_pretrained(model_path)

print("Model saved to:", model_path)

from fastapi import FastAPI, Request
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

app = FastAPI()
model = AutoModelForSequenceClassification.from_pretrained("maleeha123/SwedBERT-sentiment-v1")
tokenizer = AutoTokenizer.from_pretrained("maleeha123/SwedBERT-sentiment-v1")

from huggingface_hub import upload_folder

upload_folder(
    repo_id="maleeha123/SwedBERT-sentiment-v1",
    folder_path="SwedBERT-sentiment-v1",
    path_in_repo=".",
    token="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"  # 🔁 paste your new write token here
)

!pip install pyngrok

!python swedbert_backend.py